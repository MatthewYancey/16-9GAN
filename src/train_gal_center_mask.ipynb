{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatthewYancey/GANime/blob/master/src/train_gal_center_mask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfxMXJnq_kzW"
      },
      "source": [
        "# GANime Globally and Locally Consistent Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bouI0_AzCRZy"
      },
      "source": [
        "## Imports and Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "885wpFfz2zLh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d54c2db-b40e-4966-f29b-aee669d8d9ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import glob\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "sys.path.append('/content/gdrive/MyDrive/repos/GANime/src')\n",
        "from model_helper_functions import apply_mask, apply_padding, apply_comp, apply_scale, load_checkpoint, checkpoint, gpu_memory\n",
        "from model_data_loaders import create_dataloaders\n",
        "from model_gal import Generator, Discriminator, weights_init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rw3URZEO3MZm"
      },
      "outputs": [],
      "source": [
        "# network parameters\n",
        "BATCH_SIZE = 15\n",
        "N_EPOCHS = 100\n",
        "ALPHA_WEIGHT = 0.0004\n",
        "\n",
        "# hardware\n",
        "N_GPU = 1\n",
        "N_WORKERS = 1\n",
        "\n",
        "# image\n",
        "IMG_HEIGHT = 288\n",
        "IMG_WIDTH = 512\n",
        "SINGLE_SIDE = 64\n",
        "\n",
        "TEST_REFERENCES = [2800, 8000, 17850, 3000]\n",
        "\n",
        "# directories\n",
        "ZIP_PATH_TRAIN = '/content/gdrive/My Drive/repos/GANime/data_out/pokemon/train.zip'\n",
        "IMG_DIR_TRAIN = '/content/frames/train/'\n",
        "ZIP_PATH_VAL = '/content/gdrive/My Drive/repos/GANime/data_out/pokemon/validate.zip'\n",
        "IMG_DIR_VAL = '/content/frames/validate/'\n",
        "ZIP_PATH_TEST = '/content/gdrive/My Drive/repos/GANime/data_out/pokemon/test.zip'\n",
        "IMG_DIR_TEST = '/content/frames/test/'\n",
        "LOG_DIR = '/content/gdrive/My Drive/repos/GANime/data_out/logs/global_and_local/'\n",
        "PREV_CHECKPOINT = '/content/gdrive/My Drive/repos/GANime/data_out/logs/global_and_local/checkpoint.pt' # set to None to not load and create a new log folder\n",
        "PREV_CHECKPOINT = None # set to None to not load and create a new log folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PT61eB_hRSM4"
      },
      "outputs": [],
      "source": [
        "# unzips images\n",
        "if os.path.exists(IMG_DIR_TRAIN) == False:\n",
        "    shutil.unpack_archive(ZIP_PATH_TRAIN, IMG_DIR_TRAIN, 'zip')\n",
        "    shutil.unpack_archive(ZIP_PATH_VAL, IMG_DIR_VAL, 'zip')\n",
        "    shutil.unpack_archive(ZIP_PATH_TEST, IMG_DIR_TEST, 'zip')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# finds the dataset size and the number of batches we'll have to process\n",
        "dataset_size = len(glob.glob(f'{IMG_DIR_TRAIN}*'))\n",
        "n_batches = dataset_size // BATCH_SIZE\n",
        "print(f'Number of images: {dataset_size}')\n",
        "print(f'Number of batches: {n_batches}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcKlhcOn_bnJ",
        "outputId": "fae0d6ee-53b4-4823-e2a9-14011d6f1f98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images: 429979\n",
            "Number of batches: 28665\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiKyuXpSRBDo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ad64ac7-9347-4ff5-aeec-5f06b58e3d92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda:0\n",
            "GPU 0: Tesla V100-SXM2-16GB (UUID: GPU-580f6680-506b-7e27-c836-6e96bf8fcc39)\n"
          ]
        }
      ],
      "source": [
        "# sets what device to run on\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and N_GPU > 0) else \"cpu\")\n",
        "print(f'Device: {device}')\n",
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4NfzEuXCccW"
      },
      "source": [
        "## Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DLEwB1B5Y-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff289e0f-b709-4b4a-a43c-a4ee4a3b85fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Dataset\n",
            "Number of images: 429979\n",
            "Size of dataset: 429979\n",
            "Validation Dataset\n",
            "Number of images: 122851\n",
            "Size of dataset: 122851\n",
            "Testing Dataset\n",
            "Number of images: 61426\n",
            "Size of dataset: 61426\n"
          ]
        }
      ],
      "source": [
        "dataloader_train, dataloader_val, dataloader_test = create_dataloaders(BATCH_SIZE, N_WORKERS, IMG_DIR_TRAIN, IMG_DIR_VAL, IMG_DIR_TEST, dataset_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rphdi8N1CiiX"
      },
      "source": [
        "## Networks, Loss Functions, and Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtE6YhY53FrI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d470f545-891c-423a-8966-ae1f6e042451"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Allocated memory: 0.4209\n"
          ]
        }
      ],
      "source": [
        "gen = Generator(IMG_WIDTH, SINGLE_SIDE).to(device)\n",
        "gen.apply(weights_init)\n",
        "disc = Discriminator(IMG_WIDTH, SINGLE_SIDE).to(device)\n",
        "disc.apply(weights_init)\n",
        "gpu_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNpFXQTnZuh_"
      },
      "outputs": [],
      "source": [
        "loss_bce = nn.BCELoss()\n",
        "loss_mse = nn.MSELoss()\n",
        "optimizer_gen = optim.Adadelta(gen.parameters())\n",
        "optimizer_disc = optim.Adadelta(disc.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63H_ZHSZmFfL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2808f4b2-3b9c-42ba-cf5e-5b00c3c7d29a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folders removed\n"
          ]
        }
      ],
      "source": [
        "# loads the checkpoint\n",
        "gen, optimizer_gen, disc, optimizer_disc, batch_counter = load_checkpoint(PREV_CHECKPOINT, LOG_DIR, gen, optimizer_gen, disc, optimizer_disc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_lWXZ2i6cbd"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgZ-sLbSvWPv",
        "outputId": "184231cc-2b61-4b64-d689-5028ceed0717",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0/100, Batch: 0/28665, Total Images 0, Gen Train Loss: 0.0500, Disc Accuracy: 0.4667, CUDA Memory: 1.7648\n",
            "Saving reference images\n",
            "Saving reference images\n",
            "Saving reference images\n",
            "Saving reference images\n",
            "Saving checkpoint at new epoch\n",
            "Saved to tensorboard\n",
            "Epoch: 0/100, Batch: 100/28665, Total Images 1500, Gen Train Loss: 0.0214, Disc Accuracy: 0.5000, CUDA Memory: 2.4760\n",
            "Epoch: 0/100, Batch: 200/28665, Total Images 3000, Gen Train Loss: 0.0142, Disc Accuracy: 0.5000, CUDA Memory: 2.4760\n",
            "Epoch: 0/100, Batch: 300/28665, Total Images 4500, Gen Train Loss: 0.0239, Disc Accuracy: 0.5000, CUDA Memory: 2.4760\n",
            "Epoch: 0/100, Batch: 400/28665, Total Images 6000, Gen Train Loss: 0.0142, Disc Accuracy: 0.5000, CUDA Memory: 2.4760\n",
            "Epoch: 0/100, Batch: 500/28665, Total Images 7500, Gen Train Loss: 0.0204, Disc Accuracy: 0.4667, CUDA Memory: 2.4760\n",
            "Epoch: 0/100, Batch: 600/28665, Total Images 9000, Gen Train Loss: 0.0131, Disc Accuracy: 0.5000, CUDA Memory: 2.4760\n",
            "Epoch: 0/100, Batch: 700/28665, Total Images 10500, Gen Train Loss: 0.0157, Disc Accuracy: 0.5000, CUDA Memory: 2.4760\n",
            "Epoch: 0/100, Batch: 800/28665, Total Images 12000, Gen Train Loss: 0.0191, Disc Accuracy: 0.5000, CUDA Memory: 2.4760\n",
            "Epoch: 0/100, Batch: 900/28665, Total Images 13500, Gen Train Loss: 0.0145, Disc Accuracy: 0.5000, CUDA Memory: 2.4760\n",
            "Epoch: 0/100, Batch: 1000/28665, Total Images 15000, Gen Train Loss: 0.0154, Disc Accuracy: 0.5000, CUDA Memory: 2.4760\n",
            "Saved to tensorboard\n",
            "Epoch: 0/100, Batch: 1100/28665, Total Images 16500, Gen Train Loss: 0.0173, Disc Accuracy: 0.5333, CUDA Memory: 2.4749\n",
            "Epoch: 0/100, Batch: 1200/28665, Total Images 18000, Gen Train Loss: 0.0166, Disc Accuracy: 0.5000, CUDA Memory: 2.4749\n",
            "Epoch: 0/100, Batch: 1300/28665, Total Images 19500, Gen Train Loss: 0.0145, Disc Accuracy: 0.5000, CUDA Memory: 2.4749\n",
            "Epoch: 0/100, Batch: 1400/28665, Total Images 21000, Gen Train Loss: 0.0152, Disc Accuracy: 0.5000, CUDA Memory: 2.4749\n",
            "Epoch: 0/100, Batch: 1500/28665, Total Images 22500, Gen Train Loss: 0.0150, Disc Accuracy: 0.5333, CUDA Memory: 2.4749\n",
            "Epoch: 0/100, Batch: 1600/28665, Total Images 24000, Gen Train Loss: 0.0140, Disc Accuracy: 0.5000, CUDA Memory: 2.4749\n",
            "Epoch: 0/100, Batch: 1700/28665, Total Images 25500, Gen Train Loss: 0.0149, Disc Accuracy: 0.5333, CUDA Memory: 2.4749\n",
            "Epoch: 0/100, Batch: 1800/28665, Total Images 27000, Gen Train Loss: 0.0151, Disc Accuracy: 0.5000, CUDA Memory: 2.4749\n",
            "Epoch: 0/100, Batch: 1900/28665, Total Images 28500, Gen Train Loss: 0.0178, Disc Accuracy: 0.5000, CUDA Memory: 2.4749\n",
            "Epoch: 0/100, Batch: 2000/28665, Total Images 30000, Gen Train Loss: 0.0185, Disc Accuracy: 0.5000, CUDA Memory: 2.4749\n",
            "Saved to tensorboard\n",
            "Epoch: 0/100, Batch: 2100/28665, Total Images 31500, Gen Train Loss: 0.0134, Disc Accuracy: 0.5000, CUDA Memory: 2.4780\n",
            "Epoch: 0/100, Batch: 2200/28665, Total Images 33000, Gen Train Loss: 0.0161, Disc Accuracy: 0.5000, CUDA Memory: 2.4769\n",
            "Epoch: 0/100, Batch: 2300/28665, Total Images 34500, Gen Train Loss: 0.0144, Disc Accuracy: 0.4333, CUDA Memory: 2.4780\n",
            "Epoch: 0/100, Batch: 2400/28665, Total Images 36000, Gen Train Loss: 0.0116, Disc Accuracy: 0.4667, CUDA Memory: 2.4780\n",
            "Epoch: 0/100, Batch: 2500/28665, Total Images 37500, Gen Train Loss: 0.0107, Disc Accuracy: 0.5333, CUDA Memory: 2.4769\n",
            "Epoch: 0/100, Batch: 2600/28665, Total Images 39000, Gen Train Loss: 0.0172, Disc Accuracy: 0.4667, CUDA Memory: 2.4780\n",
            "Epoch: 0/100, Batch: 2700/28665, Total Images 40500, Gen Train Loss: 0.0142, Disc Accuracy: 0.5000, CUDA Memory: 2.4780\n",
            "Epoch: 0/100, Batch: 2800/28665, Total Images 42000, Gen Train Loss: 0.0140, Disc Accuracy: 0.7000, CUDA Memory: 2.4769\n",
            "Epoch: 0/100, Batch: 2900/28665, Total Images 43500, Gen Train Loss: 0.0148, Disc Accuracy: 0.5333, CUDA Memory: 2.4780\n",
            "Epoch: 0/100, Batch: 3000/28665, Total Images 45000, Gen Train Loss: 0.0202, Disc Accuracy: 0.4667, CUDA Memory: 2.4780\n",
            "Saved to tensorboard\n",
            "Epoch: 0/100, Batch: 3100/28665, Total Images 46500, Gen Train Loss: 0.0190, Disc Accuracy: 0.5000, CUDA Memory: 2.4760\n",
            "Epoch: 0/100, Batch: 3200/28665, Total Images 48000, Gen Train Loss: 0.0138, Disc Accuracy: 0.6333, CUDA Memory: 2.4760\n",
            "Epoch: 0/100, Batch: 3300/28665, Total Images 49500, Gen Train Loss: 0.0183, Disc Accuracy: 0.3000, CUDA Memory: 2.4760\n",
            "Epoch: 0/100, Batch: 3400/28665, Total Images 51000, Gen Train Loss: 0.0123, Disc Accuracy: 0.6000, CUDA Memory: 2.4760\n",
            "Epoch: 0/100, Batch: 3500/28665, Total Images 52500, Gen Train Loss: 0.0177, Disc Accuracy: 0.5333, CUDA Memory: 2.4760\n",
            "Epoch: 0/100, Batch: 3600/28665, Total Images 54000, Gen Train Loss: 0.0146, Disc Accuracy: 0.5000, CUDA Memory: 2.4760\n",
            "Epoch: 0/100, Batch: 3700/28665, Total Images 55500, Gen Train Loss: 0.0163, Disc Accuracy: 0.5000, CUDA Memory: 2.4760\n",
            "Epoch: 0/100, Batch: 3800/28665, Total Images 57000, Gen Train Loss: 0.0160, Disc Accuracy: 0.7667, CUDA Memory: 2.4760\n",
            "Epoch: 0/100, Batch: 3900/28665, Total Images 58500, Gen Train Loss: 0.0152, Disc Accuracy: 0.9000, CUDA Memory: 2.4760\n",
            "Epoch: 0/100, Batch: 4000/28665, Total Images 60000, Gen Train Loss: 0.0104, Disc Accuracy: 0.7667, CUDA Memory: 2.4760\n",
            "Saved to tensorboard\n",
            "Epoch: 0/100, Batch: 4100/28665, Total Images 61500, Gen Train Loss: 0.0115, Disc Accuracy: 0.8333, CUDA Memory: 2.4759\n",
            "Epoch: 0/100, Batch: 4200/28665, Total Images 63000, Gen Train Loss: 0.0171, Disc Accuracy: 0.5000, CUDA Memory: 2.4759\n",
            "Epoch: 0/100, Batch: 4300/28665, Total Images 64500, Gen Train Loss: 0.0117, Disc Accuracy: 0.7667, CUDA Memory: 2.4759\n",
            "Epoch: 0/100, Batch: 4400/28665, Total Images 66000, Gen Train Loss: 0.0145, Disc Accuracy: 0.6667, CUDA Memory: 2.4759\n",
            "Epoch: 0/100, Batch: 4500/28665, Total Images 67500, Gen Train Loss: 0.0117, Disc Accuracy: 0.8333, CUDA Memory: 2.4759\n",
            "Epoch: 0/100, Batch: 4600/28665, Total Images 69000, Gen Train Loss: 0.0115, Disc Accuracy: 0.9000, CUDA Memory: 2.4759\n",
            "Epoch: 0/100, Batch: 4700/28665, Total Images 70500, Gen Train Loss: 0.0121, Disc Accuracy: 0.5333, CUDA Memory: 2.4759\n",
            "Epoch: 0/100, Batch: 4800/28665, Total Images 72000, Gen Train Loss: 0.0121, Disc Accuracy: 0.9333, CUDA Memory: 2.4759\n",
            "Epoch: 0/100, Batch: 4900/28665, Total Images 73500, Gen Train Loss: 0.0151, Disc Accuracy: 0.9333, CUDA Memory: 2.4759\n",
            "Epoch: 0/100, Batch: 5000/28665, Total Images 75000, Gen Train Loss: 0.0129, Disc Accuracy: 0.8667, CUDA Memory: 2.4759\n",
            "Saved to tensorboard\n",
            "Epoch: 0/100, Batch: 5100/28665, Total Images 76500, Gen Train Loss: 0.0127, Disc Accuracy: 0.7667, CUDA Memory: 2.4759\n",
            "Epoch: 0/100, Batch: 5200/28665, Total Images 78000, Gen Train Loss: 0.0113, Disc Accuracy: 0.3000, CUDA Memory: 2.4759\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(N_EPOCHS):\n",
        "    # gets data for the generator\n",
        "    for i, batch in enumerate(dataloader_train, 0):\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        # apply mask to the images\n",
        "        batch_mask = batch.clone()\n",
        "        batch_mask = apply_mask(batch_mask, IMG_WIDTH, SINGLE_SIDE)\n",
        "\n",
        "        # only trains the discriminator every 9 batches\n",
        "        if i % 9 == 0:\n",
        "            #############################\n",
        "            # Discriminator\n",
        "            #############################\n",
        "            disc.zero_grad()\n",
        "            disc_output = disc(batch)\n",
        "            disc_loss_real = loss_bce(disc_output, torch.ones(disc_output.shape[0]).cuda())\n",
        "            disc_accuracy = (disc_output.round() == torch.ones(disc_output.shape[0]).cuda()).sum()\n",
        "            disc_loss_real.backward()\n",
        "\n",
        "            # passes fake images to feed the discriminator\n",
        "            gen_output = gen(batch_mask)\n",
        "            gen_output = apply_comp(batch, gen_output, IMG_WIDTH, SINGLE_SIDE)\n",
        "            disc_output = disc(gen_output) # try taking detach off\n",
        "            disc_accuracy += (disc_output.round() == torch.zeros(disc_output.shape[0]).cuda()).sum()\n",
        "            disc_accuracy = disc_accuracy / (BATCH_SIZE * 2)\n",
        "            disc_loss_fake = loss_bce(disc_output, torch.zeros(disc_output.shape[0]).to(device))\n",
        "            disc_loss_fake.backward()\n",
        "\n",
        "            # optimized the discriminator\n",
        "            disc_loss = (disc_loss_real + disc_loss_fake) / 200  # scale the loss between 0 and 1\n",
        "            optimizer_disc.step()\n",
        "\n",
        "        #############################\n",
        "        # Generater\n",
        "        #############################\n",
        "        gen.zero_grad()\n",
        "        gen_output = gen(batch_mask)\n",
        "\n",
        "        # combines the sides from the generator with the 4:3 image and calculates the mse loss against the orginal full image\n",
        "        gen_output = apply_comp(batch, gen_output, IMG_WIDTH, SINGLE_SIDE)\n",
        "        disc_output = disc(gen_output)\n",
        "        \n",
        "        # calculates the loss\n",
        "        gen_train_loss_mse = loss_mse(gen_output, batch)\n",
        "        gen_train_loss_bce = loss_bce(disc_output, torch.ones(disc_output.shape[0]).cuda())\n",
        "        gen_train_loss = (gen_train_loss_mse + gen_train_loss_bce*ALPHA_WEIGHT) / 2\n",
        "\n",
        "        # error and optimize\n",
        "        gen_train_loss.backward()\n",
        "        optimizer_gen.step()\n",
        "\n",
        "        # prints the status and checkpoints every so often\n",
        "        if i % 100 == 0:\n",
        "            print(f'Epoch: {epoch}/{N_EPOCHS}, Batch: {i}/{n_batches}, Total Images {batch_counter * BATCH_SIZE}, Gen Train Loss: {gen_train_loss:.4f}, Disc Accuracy: {disc_accuracy:.4f}, CUDA Memory: {(torch.cuda.memory_allocated() / 10**9):.4f}')\n",
        "\n",
        "            if i % 1000 == 0:\n",
        "                # gets the testing MSE\n",
        "                batch = next(iter(dataloader_val))\n",
        "                batch = batch.to(device)\n",
        "                batch_mask = batch.clone()\n",
        "                batch_mask = apply_mask(batch_mask, IMG_WIDTH, SINGLE_SIDE)\n",
        "                with torch.no_grad():\n",
        "                    gen_output = gen(batch_mask)\n",
        "                gen_output = apply_comp(batch, gen_output, IMG_WIDTH, SINGLE_SIDE)\n",
        "\n",
        "                # calculates the loss\n",
        "                disc_output = disc(gen_output)\n",
        "                val_loss_bce = loss_bce(disc_output, torch.ones(disc_output.shape[0]).cuda())\n",
        "                val_loss_mse = loss_mse(gen_output, batch)\n",
        "                val_loss = (val_loss_mse + val_loss_bce*ALPHA_WEIGHT) / 2\n",
        "\n",
        "                checkpoint(i,\n",
        "                           batch_counter,\n",
        "                           disc_loss.item(),\n",
        "                           disc_accuracy,\n",
        "                           gen_train_loss.item(),\n",
        "                           gen_train_loss_mse.item(),\n",
        "                           val_loss.item(),\n",
        "                           val_loss_mse.item(),\n",
        "                           LOG_DIR,\n",
        "                           gen,\n",
        "                           optimizer_gen,\n",
        "                           disc,\n",
        "                           optimizer_disc,\n",
        "                           dataloader_test,\n",
        "                           TEST_REFERENCES,\n",
        "                           IMG_HEIGHT,\n",
        "                           IMG_WIDTH,\n",
        "                           SINGLE_SIDE)\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        batch_counter += 1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "train_gal.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}