{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "model_GANs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatthewYancey/GANime/blob/master/src/model_GANs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfxMXJnq_kzW"
      },
      "source": [
        "# GANime GANs Model\n",
        "This notebook tests the generator network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bouI0_AzCRZy"
      },
      "source": [
        "## Imports and Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "885wpFfz2zLh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ad22ec1-9ee4-4483-8254-d3888dd51ec5"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import glob\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "sys.path.append('/content/gdrive/MyDrive/GANime/src')\n",
        "from helper_functions import apply_mask, apply_padding, apply_comp, apply_scale, load_checkpoint, checkpoint\n",
        "from data_loaders import create_dataloaders\n",
        "from networks import Generator, GlobalDiscriminator, weights_init"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw3URZEO3MZm"
      },
      "source": [
        "# network parameters\n",
        "BATCH_SIZE = 15\n",
        "DATASET_SIZE = 100000\n",
        "N_BATCHES = DATASET_SIZE // BATCH_SIZE\n",
        "N_GPU = 1\n",
        "N_WORKERS = 1\n",
        "N_EPOCHS = 100\n",
        "LEARNING_RATE = 0.0002\n",
        "\n",
        "# image\n",
        "IMG_HEIGHT = 256\n",
        "IMG_WIDTH = 455\n",
        "SINGLE_SIDE = 57\n",
        "\n",
        "# tensorboard\n",
        "TRAIN_REFERENCE_INDEX = 200\n",
        "VAL_REFERENCE_INDEX = 100\n",
        "TEST_REFERENCE_INDEX = 20\n",
        "\n",
        "# cost weights\n",
        "GAN_WEIGHT = 0.0004\n",
        "\n",
        "# directories\n",
        "ZIP_PATH_TRAIN = '/content/gdrive/My Drive/GANime/data_out/train.zip'\n",
        "IMG_DIR_TRAIN = '/content/frames/train/'\n",
        "ZIP_PATH_VAL = '/content/gdrive/My Drive/GANime/data_out/validate.zip'\n",
        "IMG_DIR_VAL = '/content/frames/validate/'\n",
        "ZIP_PATH_TEST = '/content/gdrive/My Drive/GANime/data_out/test.zip'\n",
        "IMG_DIR_TEST = '/content/frames/test/'\n",
        "LOG_DIR = '/content/gdrive/My Drive/GANime/data_out/logs/model_gans/'\n",
        "PREV_CHECKPOINT = '/content/gdrive/My Drive/GANime/data_out/logs/model_gans/checkpoint.pt' # set to none to not load and create a new log folder\n",
        "# PREV_CHECKPOINT = None # set to none to not load and create a new log folder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT61eB_hRSM4"
      },
      "source": [
        "# unzips images\n",
        "if os.path.exists(IMG_DIR_TRAIN) == False:\n",
        "    shutil.unpack_archive(ZIP_PATH_TRAIN, IMG_DIR_TRAIN, 'zip')\n",
        "    shutil.unpack_archive(ZIP_PATH_VAL, IMG_DIR_VAL, 'zip')\n",
        "    shutil.unpack_archive(ZIP_PATH_TEST, IMG_DIR_TEST, 'zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiKyuXpSRBDo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6a60de8-6c69-42a0-af3c-55f93e76bb34"
      },
      "source": [
        "# sets what device to run on\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and N_GPU > 0) else \"cpu\")\n",
        "print(f'Device: {device}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4NfzEuXCccW"
      },
      "source": [
        "## Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DLEwB1B5Y-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e223b473-d3ac-4dbd-e2f6-f7c106d23b71"
      },
      "source": [
        "dataloader_train, dataloader_val, dataloader_test = create_dataloaders(BATCH_SIZE, N_WORKERS, IMG_DIR_TRAIN, IMG_DIR_VAL, IMG_DIR_TEST, DATASET_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Dataset\n",
            "Number of images: 114808\n",
            "Size of dataset: 100000\n",
            "Validation Dataset\n",
            "Number of images: 36734\n",
            "Size of dataset: 36734\n",
            "Testing Dataset\n",
            "Number of images: 2210\n",
            "Size of dataset: 2210\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rphdi8N1CiiX"
      },
      "source": [
        "## Networks, Loss Functions, and Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtE6YhY53FrI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bed47f16-8346-4fcf-fab6-7908c69aaed8"
      },
      "source": [
        "gen = Generator(N_GPU, IMG_WIDTH, SINGLE_SIDE).to(device)\n",
        "gen.apply(weights_init)\n",
        "global_disc = GlobalDiscriminator(N_GPU).to(device)\n",
        "global_disc.apply(weights_init)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GlobalDiscriminator(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
              "  (conv2): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
              "  (conv3): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
              "  (conv4): Conv2d(256, 512, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
              "  (conv5): Conv2d(512, 512, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
              "  (conv6): Conv2d(512, 512, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
              "  (conv7): Conv2d(512, 1, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
              "  (batch64): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (batch128): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (batch256): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (batch512): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU()\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNpFXQTnZuh_"
      },
      "source": [
        "loss_bce = nn.BCELoss()\n",
        "loss_mse = nn.MSELoss()\n",
        "optimizer_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.9))\n",
        "optimizer_disc = optim.Adam(global_disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.9))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63H_ZHSZmFfL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eb03f1c-0442-4b36-ba25-327927026d1a"
      },
      "source": [
        "# loads the checkpoint\n",
        "gen, optimizer_gen, global_disc, optimizer_disc, batch_counter = load_checkpoint(PREV_CHECKPOINT, LOG_DIR, gen, optimizer_gen, global_disc, optimizer_disc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded checkpoint from /content/gdrive/My Drive/GANime/data_out/logs/model_gans/checkpoint.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_lWXZ2i6cbd"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgZ-sLbSvWPv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1764e26d-1733-4a8b-b5ef-c5b647db389b"
      },
      "source": [
        "for epoch in range(N_EPOCHS):\n",
        "    # gets data for the generator\n",
        "    for i, batch in enumerate(dataloader_train, 0):\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        #############################\n",
        "        # Discriminator\n",
        "        #############################\n",
        "        global_disc.zero_grad()\n",
        "        output_global_disc = global_disc(batch)\n",
        "        disc_loss_real = loss_bce(output_global_disc, torch.ones(output_global_disc.shape).cuda())\n",
        "        disc_loss_real.backward()\n",
        "\n",
        "        # apply mask to the images\n",
        "        batch_mask = batch.clone()\n",
        "        batch_mask = apply_mask(batch_mask, IMG_WIDTH, SINGLE_SIDE)\n",
        "\n",
        "        # passes fake images to the Discriminator\n",
        "        _, gen_output_global = gen(batch_mask)\n",
        "        gen_output_global = apply_comp(batch, gen_output_global, IMG_WIDTH, SINGLE_SIDE)\n",
        "        output_global_disc = global_disc(gen_output_global.detach())\n",
        "        disc_loss_fake = loss_bce(output_global_disc, torch.zeros(output_global_disc.shape).to(device))\n",
        "        disc_loss_fake.backward()\n",
        "\n",
        "        # optimized the discriminator\n",
        "        disc_loss = (disc_loss_real + disc_loss_fake) / 200\n",
        "        optimizer_disc.step()\n",
        "\n",
        "        #############################\n",
        "        # Generater\n",
        "        #############################\n",
        "        gen.zero_grad()\n",
        "        _, gen_output_global = gen(batch_mask)\n",
        "\n",
        "        # combines the sides from the generator with the 4:3 image and calculates the mse loss against the orginal full image\n",
        "        gen_output_global = apply_comp(batch, gen_output_global, IMG_WIDTH, SINGLE_SIDE)\n",
        "        output_global_disc = global_disc(gen_output_global)\n",
        "        \n",
        "        # calculates the loss\n",
        "        gen_train_loss_mse = loss_mse(gen_output_global, batch)\n",
        "        gen_train_loss_bce = loss_bce(output_global_disc, torch.ones(output_global_disc.shape).cuda())\n",
        "        gen_train_loss = (gen_train_loss_mse + gen_train_loss_bce * 0.01) / 2\n",
        "\n",
        "        # error and optimize\n",
        "        gen_train_loss.backward()\n",
        "        optimizer_gen.step()\n",
        "\n",
        "        # prints the status and checkpoints every so often\n",
        "        if i % 10 == 0:\n",
        "            # gets the testing MSE\n",
        "            batch = next(iter(dataloader_val))\n",
        "            batch = batch.to(device)\n",
        "            batch_mask = batch.clone()\n",
        "            batch_mask = apply_mask(batch_mask, IMG_WIDTH, SINGLE_SIDE)\n",
        "            with torch.no_grad():\n",
        "                _, gen_output_global = gen(batch_mask)\n",
        "            gen_output_global = apply_comp(batch, gen_output_global, IMG_WIDTH, SINGLE_SIDE)\n",
        "            val_loss = loss_mse(gen_output_global, batch)\n",
        "            \n",
        "            print(f'Epoch: {epoch}/{N_EPOCHS}, Batch in Epoch: {i}/{N_BATCHES}, Total Images {batch_counter * BATCH_SIZE}, Gen Train Loss: {gen_train_loss:.4f}, Gen Val Loss: {val_loss:.4f}, Disc Train Loss: {disc_loss:.4f}')\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                checkpoint(batch_counter,\n",
        "                           disc_loss.item(),\n",
        "                           gen_train_loss.item(),\n",
        "                           val_loss.item(),\n",
        "                           LOG_DIR,\n",
        "                           gen,\n",
        "                           optimizer_gen,\n",
        "                           global_disc,\n",
        "                           optimizer_disc,\n",
        "                           dataloader_train,\n",
        "                           TRAIN_REFERENCE_INDEX,\n",
        "                           dataloader_val,\n",
        "                           VAL_REFERENCE_INDEX,\n",
        "                           dataloader_test,\n",
        "                           TEST_REFERENCE_INDEX,\n",
        "                           IMG_HEIGHT,\n",
        "                           IMG_WIDTH,\n",
        "                           SINGLE_SIDE)\n",
        "\n",
        "        batch_counter += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3/100, Batch in Epoch: 9910/6666, Total Images 1639920, Gen Train Loss: 0.2586, Gen Val Loss: 0.0159, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 9920/6666, Total Images 1640070, Gen Train Loss: 0.2611, Gen Val Loss: 0.0212, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 9930/6666, Total Images 1640220, Gen Train Loss: 0.2518, Gen Val Loss: 0.0146, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 9940/6666, Total Images 1640370, Gen Train Loss: 0.1935, Gen Val Loss: 0.0293, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 9950/6666, Total Images 1640520, Gen Train Loss: 0.2631, Gen Val Loss: 0.0311, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 9960/6666, Total Images 1640670, Gen Train Loss: 0.2617, Gen Val Loss: 0.0234, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 9970/6666, Total Images 1640820, Gen Train Loss: 0.2229, Gen Val Loss: 0.0507, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 9980/6666, Total Images 1640970, Gen Train Loss: 0.2357, Gen Val Loss: 0.0132, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 9990/6666, Total Images 1641120, Gen Train Loss: 0.2067, Gen Val Loss: 0.0143, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 10000/6666, Total Images 1641270, Gen Train Loss: 0.2613, Gen Val Loss: 0.0271, Disc Train Loss: 0.0000\n",
            "Saved checkpoint\n",
            "Epoch: 3/100, Batch in Epoch: 10010/6666, Total Images 1641420, Gen Train Loss: 0.2407, Gen Val Loss: 0.0246, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 10020/6666, Total Images 1641570, Gen Train Loss: 0.2595, Gen Val Loss: 0.0258, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 10030/6666, Total Images 1641720, Gen Train Loss: 0.2643, Gen Val Loss: 0.0173, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 10040/6666, Total Images 1641870, Gen Train Loss: 0.2223, Gen Val Loss: 0.0205, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 10050/6666, Total Images 1642020, Gen Train Loss: 0.2009, Gen Val Loss: 0.0189, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 10060/6666, Total Images 1642170, Gen Train Loss: 0.2526, Gen Val Loss: 0.0238, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 10070/6666, Total Images 1642320, Gen Train Loss: 0.2629, Gen Val Loss: 0.0139, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 10080/6666, Total Images 1642470, Gen Train Loss: 0.1974, Gen Val Loss: 0.0244, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 10090/6666, Total Images 1642620, Gen Train Loss: 0.2043, Gen Val Loss: 0.0129, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 10100/6666, Total Images 1642770, Gen Train Loss: 0.2107, Gen Val Loss: 0.0314, Disc Train Loss: 0.0000\n",
            "Saved checkpoint\n",
            "Epoch: 3/100, Batch in Epoch: 10110/6666, Total Images 1642920, Gen Train Loss: 0.1835, Gen Val Loss: 0.0111, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 10120/6666, Total Images 1643070, Gen Train Loss: 0.2753, Gen Val Loss: 0.0248, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 10130/6666, Total Images 1643220, Gen Train Loss: 0.2664, Gen Val Loss: 0.0170, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 10140/6666, Total Images 1643370, Gen Train Loss: 0.2722, Gen Val Loss: 0.0096, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 10150/6666, Total Images 1643520, Gen Train Loss: 0.2559, Gen Val Loss: 0.0228, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 10160/6666, Total Images 1643670, Gen Train Loss: 0.2398, Gen Val Loss: 0.0139, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 10170/6666, Total Images 1643820, Gen Train Loss: 0.2547, Gen Val Loss: 0.0177, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 10180/6666, Total Images 1643970, Gen Train Loss: 0.2345, Gen Val Loss: 0.0317, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 10190/6666, Total Images 1644120, Gen Train Loss: 0.2662, Gen Val Loss: 0.0256, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 10200/6666, Total Images 1644270, Gen Train Loss: 0.2595, Gen Val Loss: 0.0235, Disc Train Loss: 0.0000\n",
            "Saved checkpoint\n",
            "Epoch: 3/100, Batch in Epoch: 10210/6666, Total Images 1644420, Gen Train Loss: 0.2481, Gen Val Loss: 0.0222, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 10220/6666, Total Images 1644570, Gen Train Loss: 0.1971, Gen Val Loss: 0.0349, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 10230/6666, Total Images 1644720, Gen Train Loss: 0.1928, Gen Val Loss: 0.0190, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 10240/6666, Total Images 1644870, Gen Train Loss: 0.2346, Gen Val Loss: 0.0246, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 10250/6666, Total Images 1645020, Gen Train Loss: 0.2449, Gen Val Loss: 0.0155, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 10260/6666, Total Images 1645170, Gen Train Loss: 0.2349, Gen Val Loss: 0.0238, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 10270/6666, Total Images 1645320, Gen Train Loss: 0.2422, Gen Val Loss: 0.0232, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 10280/6666, Total Images 1645470, Gen Train Loss: 0.2356, Gen Val Loss: 0.0250, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 10290/6666, Total Images 1645620, Gen Train Loss: 0.2675, Gen Val Loss: 0.0243, Disc Train Loss: 0.0000\n",
            "Epoch: 3/100, Batch in Epoch: 10300/6666, Total Images 1645770, Gen Train Loss: 0.2592, Gen Val Loss: 0.0109, Disc Train Loss: 0.0000\n",
            "Saved checkpoint\n"
          ]
        }
      ]
    }
  ]
}