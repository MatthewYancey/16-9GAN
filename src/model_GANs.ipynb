{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "model_GANs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatthewYancey/GANime/blob/master/src/model_GANs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfxMXJnq_kzW"
      },
      "source": [
        "# GANime GANs Model\n",
        "This notebook tests the generator network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bouI0_AzCRZy"
      },
      "source": [
        "## Imports and Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "885wpFfz2zLh"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import glob\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "sys.path.append('/content/gdrive/MyDrive/GANime/src')\n",
        "from helper_functions import apply_mask, apply_padding, apply_comp, apply_scale, load_checkpoint, checkpoint\n",
        "from data_loaders import create_dataloaders\n",
        "from networks import Generator, GlobalDiscriminator, weights_init"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw3URZEO3MZm"
      },
      "source": [
        "# network parameters\n",
        "BATCH_SIZE = 15\n",
        "DATASET_SIZE = 100000\n",
        "N_BATCHES = DATASET_SIZE // BATCH_SIZE\n",
        "N_GPU = 1\n",
        "N_WORKERS = 1\n",
        "N_EPOCHS = 100\n",
        "LEARNING_RATE = 0.0002\n",
        "\n",
        "# image\n",
        "IMG_HEIGHT = 256\n",
        "IMG_WIDTH = 455\n",
        "SINGLE_SIDE = 57\n",
        "\n",
        "# tensorboard\n",
        "TRAIN_REFERENCE_INDEX = 200\n",
        "VAL_REFERENCE_INDEX = 100\n",
        "TEST_REFERENCE_INDEX = 20\n",
        "\n",
        "# directories\n",
        "ZIP_PATH_TRAIN = '/content/gdrive/My Drive/GANime/data_out/train.zip'\n",
        "IMG_DIR_TRAIN = '/content/frames/train/'\n",
        "ZIP_PATH_VAL = '/content/gdrive/My Drive/GANime/data_out/validate.zip'\n",
        "IMG_DIR_VAL = '/content/frames/validate/'\n",
        "ZIP_PATH_TEST = '/content/gdrive/My Drive/GANime/data_out/test.zip'\n",
        "IMG_DIR_TEST = '/content/frames/test/'\n",
        "LOG_DIR = '/content/gdrive/My Drive/GANime/data_out/logs/temp2/'\n",
        "PREV_CHECKPOINT = '/content/gdrive/My Drive/GANime/data_out/logs/model_mse/checkpoint.pt' # set to none to not load and create a new log folder\n",
        "# PREV_CHECKPOINT = None # set to none to not load and create a new log folder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT61eB_hRSM4"
      },
      "source": [
        "# unzips images\n",
        "if os.path.exists(IMG_DIR_TRAIN) == False:\n",
        "    shutil.unpack_archive(ZIP_PATH_TRAIN, IMG_DIR_TRAIN, 'zip')\n",
        "    shutil.unpack_archive(ZIP_PATH_VAL, IMG_DIR_VAL, 'zip')\n",
        "    shutil.unpack_archive(ZIP_PATH_TEST, IMG_DIR_TEST, 'zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiKyuXpSRBDo"
      },
      "source": [
        "# sets what device to run on\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and N_GPU > 0) else \"cpu\")\n",
        "print(f'Device: {device}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4NfzEuXCccW"
      },
      "source": [
        "## Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DLEwB1B5Y-6"
      },
      "source": [
        "dataloader_train, dataloader_val, dataloader_test = create_dataloaders(BATCH_SIZE, N_WORKERS, IMG_DIR_TRAIN, IMG_DIR_VAL, IMG_DIR_TEST, DATASET_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rphdi8N1CiiX"
      },
      "source": [
        "## Networks, Loss Functions, and Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtE6YhY53FrI"
      },
      "source": [
        "gen = Generator(N_GPU, IMG_WIDTH, SINGLE_SIDE).to(device)\n",
        "gen.apply(weights_init)\n",
        "global_disc = GlobalDiscriminator(N_GPU).to(device)\n",
        "global_disc.apply(weights_init)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNpFXQTnZuh_"
      },
      "source": [
        "loss_bce = nn.BCELoss()\n",
        "loss_mse = nn.MSELoss()\n",
        "optimizer_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.9))\n",
        "optimizer_disc = optim.Adam(global_disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.9))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63H_ZHSZmFfL"
      },
      "source": [
        "# loads the checkpoint\n",
        "gen, optimizer_gen, batch_counter = load_checkpoint(PREV_CHECKPOINT, LOG_DIR, gen, optimizer_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_lWXZ2i6cbd"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgZ-sLbSvWPv"
      },
      "source": [
        "for epoch in range(N_EPOCHS):\n",
        "    # gets data for the generator\n",
        "    for i, batch in enumerate(dataloader_train, 0):\n",
        "\n",
        "        #############################\n",
        "        # Discriminator\n",
        "        #############################\n",
        "        global_disc.zero_grad()\n",
        "        output_global_disc = global_disc(batch.to(device))\n",
        "        disc_loss_real = loss_bce(output_global_disc, torch.ones(output_global_disc.shape).cuda())\n",
        "        disc_loss_real.backward()\n",
        "\n",
        "        # apply mask to the images\n",
        "        batch_mask = batch.clone()\n",
        "        batch_mask = apply_mask(batch_mask, IMG_WIDTH, SINGLE_SIDE)\n",
        "\n",
        "        # passes fake images to the Discriminator\n",
        "        gen_output_local, gen_output_global = gen(batch_mask.to(device))\n",
        "        output_global_disc = global_disc(gen_output_global.detach())\n",
        "        disc_loss_fake = loss_bce(output_global_disc, torch.zeros(output_global_disc.shape).to(device))\n",
        "        disc_loss_fake.backward()\n",
        "\n",
        "        # optimized the discriminator\n",
        "        disc_loss = disc_loss_real + disc_loss_fake\n",
        "        optimizer_disc.step()\n",
        "\n",
        "        #############################\n",
        "        # Generater\n",
        "        #############################\n",
        "        gen.zero_grad()\n",
        "        _, gen_output_global = gen(batch_mask.to(device))\n",
        "        # output_global_disc = global_disc()\n",
        "\n",
        "        # combines the sides from the generator with the original 4:3 and calculates the mse loss against the orginal image\n",
        "        gen_output_global = apply_comp(batch.to(device), gen_output_global, IMG_WIDTH, SINGLE_SIDE)\n",
        "        gen_train_loss_mse = loss_mse(gen_output_global, batch.to(device))\n",
        "        \n",
        "        gen_train_loss = gen_train_loss_mse \n",
        "\n",
        "        # error and optimize\n",
        "        gen_train_loss.backward()\n",
        "        optimizer_gen.step()\n",
        "\n",
        "        # prints the status and checkpoints every so often\n",
        "        if i % 10 == 0:\n",
        "            # gets the testing MSE\n",
        "            batch = next(iter(dataloader_val))\n",
        "            batch_mask = batch.clone()\n",
        "            batch_mask = apply_mask(batch_mask, IMG_WIDTH, SINGLE_SIDE)\n",
        "            with torch.no_grad():\n",
        "                _, gen_output_global = gen(batch_mask.to(device))\n",
        "            gen_output_global = apply_comp(batch.to(device), gen_output_global, IMG_WIDTH, SINGLE_SIDE)\n",
        "            val_loss = loss_mse(gen_output_global, batch.to(device))\n",
        "            \n",
        "            print(f'Epoch: {epoch}/{N_EPOCHS}, Batch in Epoch: {i}/{N_BATCHES}, Total Images {batch_counter * BATCH_SIZE}, Gen Train Loss: {gen_train_loss:.4f}, Gen Val Loss: {val_loss:.4f}, Disc Train Loss: {disc_loss:.4f}')\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                checkpoint(batch_counter,\n",
        "                           disc_loss.item(),\n",
        "                           gen_train_loss.item(),\n",
        "                           val_loss.item(),\n",
        "                           LOG_DIR,\n",
        "                           gen,\n",
        "                           optimizer_gen,\n",
        "                           dataloader_train,\n",
        "                           TRAIN_REFERENCE_INDEX,\n",
        "                           dataloader_val,\n",
        "                           VAL_REFERENCE_INDEX,\n",
        "                           dataloader_test,\n",
        "                           TEST_REFERENCE_INDEX,\n",
        "                           IMG_HEIGHT,\n",
        "                           IMG_WIDTH,\n",
        "                           SINGLE_SIDE)\n",
        "\n",
        "        batch_counter += 1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}