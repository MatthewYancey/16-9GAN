{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "model_global_and_local_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatthewYancey/GANime/blob/master/src/model_global_and_local_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfxMXJnq_kzW"
      },
      "source": [
        "# GANime Globally and Locally Consistent Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bouI0_AzCRZy"
      },
      "source": [
        "## Imports and Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "885wpFfz2zLh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eabed12-8817-4227-f781-446ef7aebad4"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import glob\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "sys.path.append('/content/gdrive/MyDrive/repos/GANime/src')\n",
        "from helper_functions import apply_mask, apply_padding, apply_comp, apply_scale, load_checkpoint, checkpoint\n",
        "from data_loaders import create_dataloaders\n",
        "from networks_global_and_local import Generator, Discriminator, weights_init"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw3URZEO3MZm"
      },
      "source": [
        "# network parameters\n",
        "BATCH_SIZE = 15\n",
        "DATASET_SIZE = 100000\n",
        "N_BATCHES = DATASET_SIZE // BATCH_SIZE\n",
        "N_GPU = 1\n",
        "N_WORKERS = 1\n",
        "N_EPOCHS = 100\n",
        "LEARNING_RATE = 0.0002\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "# image\n",
        "IMG_HEIGHT = 288\n",
        "IMG_WIDTH = 512\n",
        "SINGLE_SIDE = 64\n",
        "\n",
        "# tensorboard\n",
        "TRAIN_REFERENCE_INDEX = 200\n",
        "VAL_REFERENCE_INDEX = 100\n",
        "TEST_REFERENCE_INDEX = 20\n",
        "\n",
        "# cost weights\n",
        "WEIGHT_DECAY = 0.05\n",
        "\n",
        "# directories\n",
        "ZIP_PATH_TRAIN = '/content/gdrive/My Drive/repos/GANime/data_out/pokemon/train.zip'\n",
        "IMG_DIR_TRAIN = '/content/frames/train/'\n",
        "ZIP_PATH_VAL = '/content/gdrive/My Drive/repos/GANime/data_out/pokemon/validate.zip'\n",
        "IMG_DIR_VAL = '/content/frames/validate/'\n",
        "ZIP_PATH_TEST = '/content/gdrive/My Drive/repos/GANime/data_out/pokemon/test.zip'\n",
        "IMG_DIR_TEST = '/content/frames/test/'\n",
        "LOG_DIR = '/content/gdrive/My Drive/repos/GANime/data_out/logs/global_and_local/'\n",
        "# PREV_CHECKPOINT = '/content/gdrive/My Drive/repos/GANime/data_out/logs/global_and_local/checkpoint.pt' # set to none to not load and create a new log folder\n",
        "PREV_CHECKPOINT = None # set to none to not load and create a new log folder"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT61eB_hRSM4"
      },
      "source": [
        "# unzips images\n",
        "if os.path.exists(IMG_DIR_TRAIN) == False:\n",
        "    shutil.unpack_archive(ZIP_PATH_TRAIN, IMG_DIR_TRAIN, 'zip')\n",
        "    shutil.unpack_archive(ZIP_PATH_VAL, IMG_DIR_VAL, 'zip')\n",
        "    shutil.unpack_archive(ZIP_PATH_TEST, IMG_DIR_TEST, 'zip')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiKyuXpSRBDo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2053712-f10c-43e8-ced3-49eb9dbe2f5e"
      },
      "source": [
        "# sets what device to run on\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and N_GPU > 0) else \"cpu\")\n",
        "print(f'Device: {device}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4NfzEuXCccW"
      },
      "source": [
        "## Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DLEwB1B5Y-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90a04a5e-5d80-4d5d-8746-7de3c9930805"
      },
      "source": [
        "dataloader_train, dataloader_val, dataloader_test = create_dataloaders(BATCH_SIZE, N_WORKERS, IMG_DIR_TRAIN, IMG_DIR_VAL, IMG_DIR_TEST, DATASET_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Dataset\n",
            "Number of images: 114808\n",
            "Size of dataset: 100000\n",
            "Validation Dataset\n",
            "Number of images: 36734\n",
            "Size of dataset: 36734\n",
            "Testing Dataset\n",
            "Number of images: 2210\n",
            "Size of dataset: 2210\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rphdi8N1CiiX"
      },
      "source": [
        "## Networks, Loss Functions, and Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtE6YhY53FrI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6994de1b-b3cd-40c6-e0b3-406a4c67003c"
      },
      "source": [
        "gen = Generator(IMG_WIDTH, SINGLE_SIDE).to(device)\n",
        "gen.apply(weights_init)\n",
        "disc = Discriminator(IMG_WIDTH, SINGLE_SIDE, DROPOUT_RATE).to(device)\n",
        "disc.apply(weights_init)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (global_disc): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (1): Dropout(p=0.2, inplace=False)\n",
              "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): ReLU()\n",
              "    (4): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (5): Dropout(p=0.2, inplace=False)\n",
              "    (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): ReLU()\n",
              "    (8): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (9): Dropout(p=0.2, inplace=False)\n",
              "    (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (11): ReLU()\n",
              "    (12): Conv2d(256, 512, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (13): Dropout(p=0.2, inplace=False)\n",
              "    (14): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (15): ReLU()\n",
              "    (16): Conv2d(512, 512, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (17): Dropout(p=0.2, inplace=False)\n",
              "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (19): ReLU()\n",
              "    (20): Conv2d(512, 512, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (21): Dropout(p=0.2, inplace=False)\n",
              "    (22): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (23): ReLU()\n",
              "  )\n",
              "  (local_disc): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (1): Dropout(p=0.2, inplace=False)\n",
              "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): ReLU()\n",
              "    (4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (5): Dropout(p=0.2, inplace=False)\n",
              "    (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): ReLU()\n",
              "    (8): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (9): Dropout(p=0.2, inplace=False)\n",
              "    (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (11): ReLU()\n",
              "    (12): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (13): Dropout(p=0.2, inplace=False)\n",
              "    (14): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (15): ReLU()\n",
              "    (16): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (17): Dropout(p=0.2, inplace=False)\n",
              "    (18): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (19): ReLU()\n",
              "  )\n",
              "  (concat_net): Sequential(\n",
              "    (0): Linear(in_features=17408, out_features=1, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNpFXQTnZuh_"
      },
      "source": [
        "loss_bce = nn.BCELoss()\n",
        "loss_mse = nn.MSELoss()\n",
        "optimizer_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.9))\n",
        "optimizer_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.9), weight_decay=WEIGHT_DECAY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63H_ZHSZmFfL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a00721c6-d11c-4826-dfa4-335c24762dbb"
      },
      "source": [
        "# loads the checkpoint\n",
        "gen, optimizer_gen, disc, optimizer_disc, batch_counter = load_checkpoint(PREV_CHECKPOINT, LOG_DIR, gen, optimizer_gen, disc, optimizer_disc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded checkpoint from /content/gdrive/My Drive/repos/GANime/data_out/logs/model_gans/checkpoint.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_lWXZ2i6cbd"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgZ-sLbSvWPv"
      },
      "source": [
        "for epoch in range(N_EPOCHS):\n",
        "    # gets data for the generator\n",
        "    for i, batch in enumerate(dataloader_train, 0):\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        #############################\n",
        "        # Discriminator\n",
        "        #############################\n",
        "        disc.zero_grad()\n",
        "        disc_output = disc(batch)\n",
        "        disc_loss_real = loss_bce(disc_output, torch.ones(disc_output.shape[0]).cuda())\n",
        "        disc_loss_real.backward()\n",
        "\n",
        "        # apply mask to the images\n",
        "        batch_mask = batch.clone()\n",
        "        batch_mask = apply_mask(batch_mask, IMG_WIDTH, SINGLE_SIDE)\n",
        "\n",
        "        # passes fake images to feed the discriminator\n",
        "        gen_output = gen(batch_mask)\n",
        "        gen_output = apply_comp(batch, gen_output, IMG_WIDTH, SINGLE_SIDE)\n",
        "        disc_output = disc(gen_output) # try taking detach off\n",
        "        disc_loss_fake = loss_bce(disc_output, torch.zeros(disc_output.shape[0]).to(device))\n",
        "        disc_loss_fake.backward()\n",
        "\n",
        "        # optimized the discriminator\n",
        "        disc_loss = (disc_loss_real + disc_loss_fake) / 200  # scale the loss between 0 and 1\n",
        "        optimizer_disc.step()\n",
        "\n",
        "        #############################\n",
        "        # Generater\n",
        "        #############################\n",
        "        gen.zero_grad()\n",
        "        gen_output = gen(batch_mask)\n",
        "\n",
        "        # combines the sides from the generator with the 4:3 image and calculates the mse loss against the orginal full image\n",
        "        gen_output = apply_comp(batch, gen_output, IMG_WIDTH, SINGLE_SIDE)\n",
        "        disc_output = disc(gen_output)\n",
        "        \n",
        "        # calculates the loss\n",
        "        gen_train_loss_mse = loss_mse(gen_output, batch)\n",
        "        gen_train_loss_bce = loss_bce(disc_output, torch.ones(disc_output.shape[0]).cuda())\n",
        "        gen_train_loss = (gen_train_loss_mse + gen_train_loss_bce * 0.01) / 2\n",
        "\n",
        "        # error and optimize\n",
        "        gen_train_loss.backward()\n",
        "        optimizer_gen.step()\n",
        "\n",
        "        # prints the status and checkpoints every so often\n",
        "        if i % 10 == 0:\n",
        "            # gets the testing MSE\n",
        "            batch = next(iter(dataloader_val))\n",
        "            batch = batch.to(device)\n",
        "            batch_mask = batch.clone()\n",
        "            batch_mask = apply_mask(batch_mask, IMG_WIDTH, SINGLE_SIDE)\n",
        "            with torch.no_grad():\n",
        "                gen_output = gen(batch_mask)\n",
        "            gen_output = apply_comp(batch, gen_output, IMG_WIDTH, SINGLE_SIDE)\n",
        "            val_loss = loss_mse(gen_output, batch)\n",
        "            \n",
        "            print(f'Epoch: {epoch}/{N_EPOCHS}, Batch in Epoch: {i}/{N_BATCHES}, Total Images {batch_counter * BATCH_SIZE}, Gen Train Loss: {gen_train_loss:.4f}, Gen Val Loss: {val_loss:.4f}, Disc Train Loss: {disc_loss:.4f}')\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                checkpoint(batch_counter,\n",
        "                           disc_loss.item(),\n",
        "                           gen_train_loss.item(),\n",
        "                           val_loss.item(),\n",
        "                           LOG_DIR,\n",
        "                           gen,\n",
        "                           optimizer_gen,\n",
        "                           disc,\n",
        "                           optimizer_disc,\n",
        "                           dataloader_train,\n",
        "                           TRAIN_REFERENCE_INDEX,\n",
        "                           dataloader_val,\n",
        "                           VAL_REFERENCE_INDEX,\n",
        "                           dataloader_test,\n",
        "                           TEST_REFERENCE_INDEX,\n",
        "                           IMG_HEIGHT,\n",
        "                           IMG_WIDTH,\n",
        "                           SINGLE_SIDE)\n",
        "\n",
        "        batch_counter += 1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}