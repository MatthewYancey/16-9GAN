{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_context_encoders.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNY4HYgl+INj5KorqFanKLE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatthewYancey/GANime/blob/master/src/model_context_encoders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfxMXJnq_kzW"
      },
      "source": [
        "# GANime - Context Encoder\n",
        "This notebook applies the context encoder paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bouI0_AzCRZy"
      },
      "source": [
        "## Imports and Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "885wpFfz2zLh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c76ad44-8f17-49d8-ea56-e97d9e1fa260"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import glob\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "sys.path.append('/content/gdrive/MyDrive/repos/GANime/src')\n",
        "from helper_functions import apply_mask, apply_padding, apply_comp, apply_scale, load_checkpoint, checkpoint, gpu_memory\n",
        "from data_loaders import create_dataloaders\n",
        "from networks_context_encoders import Generator, Discriminator, weights_init"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw3URZEO3MZm"
      },
      "source": [
        "# network parameters\n",
        "BATCH_SIZE = 15\n",
        "DATASET_SIZE = 1000000\n",
        "N_BATCHES = DATASET_SIZE // BATCH_SIZE\n",
        "N_GPU = 1\n",
        "N_WORKERS = 1\n",
        "N_EPOCHS = 100\n",
        "GEN_LEARNING_RATE = 0.001\n",
        "DISC_LEARNING_RATE = 0.0001\n",
        "ALPHA_WEIGHT = 0.999\n",
        "DROPOUT_RATE = 0.1\n",
        "\n",
        "# image\n",
        "IMG_HEIGHT = 288\n",
        "IMG_WIDTH = 512\n",
        "SINGLE_SIDE = 64\n",
        "\n",
        "# tensorboard\n",
        "# TRAIN_REFERENCE_INDEX = 200\n",
        "# VAL_REFERENCE_INDEX = 100\n",
        "TEST_REFERENCES = [2800, 8000, 17850, 3000]\n",
        "\n",
        "# cost weights\n",
        "WEIGHT_DECAY = 0.05\n",
        "\n",
        "# directories\n",
        "ZIP_PATH_TRAIN = '/content/gdrive/My Drive/repos/GANime/data_out/pokemon/train.zip'\n",
        "IMG_DIR_TRAIN = '/content/frames/train/'\n",
        "ZIP_PATH_VAL = '/content/gdrive/My Drive/repos/GANime/data_out/pokemon/validate.zip'\n",
        "IMG_DIR_VAL = '/content/frames/validate/'\n",
        "ZIP_PATH_TEST = '/content/gdrive/My Drive/repos/GANime/data_out/pokemon/test.zip'\n",
        "IMG_DIR_TEST = '/content/frames/test/'\n",
        "LOG_DIR = '/content/gdrive/My Drive/repos/GANime/data_out/logs/model_context_encoders/'\n",
        "PREV_CHECKPOINT = '/content/gdrive/My Drive/repos/GANime/data_out/logs/model_context_encoders/checkpoint.pt' # set to none to not load and create a new log folder\n",
        "# PREV_CHECKPOINT = None # set to none to not load and create a new log folder"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unzips images\n",
        "if os.path.exists(IMG_DIR_TRAIN) == False:\n",
        "    shutil.unpack_archive(ZIP_PATH_TRAIN, IMG_DIR_TRAIN, 'zip')\n",
        "    shutil.unpack_archive(ZIP_PATH_VAL, IMG_DIR_VAL, 'zip')\n",
        "    shutil.unpack_archive(ZIP_PATH_TEST, IMG_DIR_TEST, 'zip')"
      ],
      "metadata": {
        "id": "lM3AjT6K7gHK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sets what device to run on\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and N_GPU > 0) else \"cpu\")\n",
        "print(f'Device: {device}')\n",
        "!nvidia-smi -L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0tVJnSA72ht",
        "outputId": "d722ad5f-cbcd-4536-9a99-eca0d44325eb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda:0\n",
            "GPU 0: Tesla V100-SXM2-16GB (UUID: GPU-ed9a6da3-733c-fcb8-5afb-96e175173c35)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader_train, dataloader_val, dataloader_test = create_dataloaders(BATCH_SIZE, N_WORKERS, IMG_DIR_TRAIN, IMG_DIR_VAL, IMG_DIR_TEST, DATASET_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mi4wVo_075gD",
        "outputId": "5068607b-5370-4ba3-ec8a-dd7e2c98220d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Dataset\n",
            "Number of images: 429979\n",
            "Size of dataset: 429979\n",
            "Validation Dataset\n",
            "Number of images: 122851\n",
            "Size of dataset: 122851\n",
            "Testing Dataset\n",
            "Number of images: 61426\n",
            "Size of dataset: 61426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gen = Generator(IMG_WIDTH, SINGLE_SIDE).to(device)\n",
        "gen.apply(weights_init)\n",
        "disc = Discriminator().to(device)\n",
        "disc.apply(weights_init)\n",
        "gpu_memory()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wu0EwLAf78i2",
        "outputId": "e4d14351-300f-478b-c096-1a612e485ab1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Allocated memory: 2.3949\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_bce = nn.BCELoss()\n",
        "loss_mse = nn.MSELoss()\n",
        "optimizer_gen = optim.Adam(gen.parameters(), lr=GEN_LEARNING_RATE, betas=(0.5, 0.9))\n",
        "optimizer_disc = optim.Adam(disc.parameters(), lr=DISC_LEARNING_RATE, betas=(0.5, 0.9), weight_decay=WEIGHT_DECAY)"
      ],
      "metadata": {
        "id": "EQTRkGdX-vtX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loads the checkpoint\n",
        "gen, optimizer_gen, disc, optimizer_disc, batch_counter = load_checkpoint(PREV_CHECKPOINT, LOG_DIR, gen, optimizer_gen, disc, optimizer_disc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jrn_SZo4_m_-",
        "outputId": "b0d5813b-4ff3-41dd-ab28-65d90eb1b04c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Allocated memory: 2.3949\n",
            "Loaded checkpoint from /content/gdrive/My Drive/repos/GANime/data_out/logs/model_context_encoders/checkpoint.pt\n",
            "Allocated memory: 7.1847\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(N_EPOCHS):\n",
        "    # gets data for the generator\n",
        "    for i, batch in enumerate(dataloader_train, 0):\n",
        "        batch = batch.to(device)\n",
        "        #############################\n",
        "        # Discriminator\n",
        "        #############################\n",
        "        disc.zero_grad()\n",
        "        disc_output = disc(batch)\n",
        "        disc_loss_real = loss_bce(disc_output, torch.ones(disc_output.shape[0], 1).cuda())\n",
        "        disc_accuracy = (disc_output.round() == torch.ones(disc_output.shape[0], 1).cuda()).sum()\n",
        "        disc_loss_real.backward()\n",
        "\n",
        "        # apply mask to the images\n",
        "        batch_mask = batch.clone()\n",
        "        batch_mask = apply_mask(batch_mask, IMG_WIDTH, SINGLE_SIDE)\n",
        "\n",
        "        # passes fake images to feed the discriminator\n",
        "        gen_output = gen(batch_mask)\n",
        "        gen_output = apply_comp(batch, gen_output, IMG_WIDTH, SINGLE_SIDE)\n",
        "        disc_output = disc(gen_output)\n",
        "\n",
        "        # calcualtes accuracy and loss on the fake images\n",
        "        disc_accuracy += (disc_output.round() == torch.zeros(disc_output.shape[0], 1).cuda()).sum()\n",
        "        disc_accuracy = disc_accuracy / (BATCH_SIZE * 2)\n",
        "        disc_loss_fake = loss_bce(disc_output, torch.zeros(disc_output.shape[0], 1).to(device))\n",
        "        disc_loss_fake.backward()\n",
        "\n",
        "        # optimized the discriminator\n",
        "        disc_loss = (disc_loss_real + disc_loss_fake) / 200  # scale the loss between 0 and 1\n",
        "        optimizer_disc.step()\n",
        "\n",
        "        #############################\n",
        "        # Generater\n",
        "        #############################\n",
        "        gen.zero_grad()\n",
        "        gen_output = gen(batch_mask)\n",
        "\n",
        "        # combines the sides from the generator with the 4:3\n",
        "        gen_output = apply_comp(batch, gen_output, IMG_WIDTH, SINGLE_SIDE)\n",
        "        disc_output = disc(gen_output)\n",
        "        \n",
        "        # calculates the loss\n",
        "        gen_train_loss_l2 = loss_mse(gen_output, batch)\n",
        "        gen_train_loss_bce = loss_bce(disc_output, torch.ones(disc_output.shape[0], 1).cuda())\n",
        "        gen_train_loss = gen_train_loss_l2*ALPHA_WEIGHT + gen_train_loss_bce*(1-ALPHA_WEIGHT)\n",
        "\n",
        "        # error and optimize\n",
        "        gen_train_loss.backward()\n",
        "        optimizer_gen.step()\n",
        "\n",
        "        # prints the status and checkpoints every so often\n",
        "        if i % 100 == 0:\n",
        "            print(f'Epoch: {epoch}/{N_EPOCHS}, Batch: {i}/{N_BATCHES}, Total Images {batch_counter * BATCH_SIZE}, Gen Train Loss: {gen_train_loss:.4f}, Disc Accuracy: {disc_accuracy:.4f}, CUDA Memory: {(torch.cuda.memory_allocated() / 10**9):.4f}')\n",
        "\n",
        "            if i % 200 == 0:\n",
        "\n",
        "                # gets the validation loss\n",
        "                batch = next(iter(dataloader_val))\n",
        "                batch = batch.to(device)\n",
        "                batch_mask = batch.clone()\n",
        "                batch_mask = apply_mask(batch_mask, IMG_WIDTH, SINGLE_SIDE)\n",
        "                with torch.no_grad():\n",
        "                    gen_output = gen(batch_mask)\n",
        "                gen_output = apply_comp(batch, gen_output, IMG_WIDTH, SINGLE_SIDE)\n",
        "\n",
        "                # calcuates the validation loss\n",
        "                disc_output = disc(gen_output)\n",
        "                gen_val_loss_bce = loss_bce(disc_output, torch.ones(disc_output.shape[0], 1).cuda())\n",
        "                gen_val_loss_l2 = loss_mse(gen_output, batch)\n",
        "                gen_val_loss = gen_val_loss_l2*ALPHA_WEIGHT + gen_val_loss_bce*(1-ALPHA_WEIGHT)\n",
        "\n",
        "                torch.cuda.empty_cache()\n",
        "                checkpoint(i,\n",
        "                           batch_counter,\n",
        "                           disc_loss.item(),\n",
        "                           disc_accuracy,\n",
        "                           gen_train_loss.item(),\n",
        "                           gen_train_loss_l2.item(),\n",
        "                           gen_val_loss.item(),\n",
        "                           gen_val_loss_l2.item(),\n",
        "                           LOG_DIR,\n",
        "                           gen,\n",
        "                           optimizer_gen,\n",
        "                           disc,\n",
        "                           optimizer_disc,\n",
        "                           dataloader_test,\n",
        "                           TEST_REFERENCES,\n",
        "                           IMG_HEIGHT,\n",
        "                           IMG_WIDTH,\n",
        "                           SINGLE_SIDE)\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        batch_counter += 1"
      ],
      "metadata": {
        "id": "uCycYGQeAHRV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}