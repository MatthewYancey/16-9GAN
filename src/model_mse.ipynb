{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "model_gans_right_side.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatthewYancey/GANime/blob/master/src/model_mse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfxMXJnq_kzW"
      },
      "source": [
        "# 16:9 Generative Model\n",
        "This notebook takes images that are in a 4:3 aspect ratio and converts them to 16:9 aspect ratio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bouI0_AzCRZy"
      },
      "source": [
        "## Imports and Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "885wpFfz2zLh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec08ad6d-60b8-4ad1-b3af-f75a4f5fb6fe"
      },
      "source": [
        "import shutil\n",
        "import glob\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw3URZEO3MZm"
      },
      "source": [
        "# network parameters\n",
        "BATCH_SIZE = 30\n",
        "DATASET_SIZE = 100000\n",
        "N_BATCHES = (DATASET_SIZE // BATCH_SIZE) * 2 # divide by two because we have two data loaders and the batch size is split between them\n",
        "N_GPU = 1\n",
        "N_WORKERS = 1\n",
        "N_EPOCHS = 100\n",
        "LEARNING_RATE = 0.0002\n",
        "\n",
        "# image\n",
        "IMG_HEIGHT = 256\n",
        "IMG_WIDTH = 455\n",
        "SINGLE_SIDE = 57\n",
        "# IMG_WIDTH = 256\n",
        "# SINGLE_SIDE = 32\n",
        "\n",
        "# directories\n",
        "ZIP_PATH_TRAIN = '/content/gdrive/My Drive/GANime/data_out/train.zip'\n",
        "IMG_DIR_TRAIN = '/content/frames/train/'\n",
        "ZIP_PATH_VAL = '/content/gdrive/My Drive/GANime/data_out/validate.zip'\n",
        "IMG_DIR_VAL = '/content/frames/validate/'\n",
        "ZIP_PATH_TEST = '/content/gdrive/My Drive/GANime/data_out/test.zip'\n",
        "IMG_DIR_TEST = '/content/frames/test/'\n",
        "LOG_DIR = '/content/gdrive/My Drive/GANime/data_out/logs/model_001/'\n",
        "\n",
        "# checkpoint type\n",
        "CHECKPOINT_TYPE = 'prev_checkpoint' # none or prev_checkpoint"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT61eB_hRSM4"
      },
      "source": [
        "# unzips images\n",
        "shutil.unpack_archive(ZIP_PATH_TRAIN, IMG_DIR_TRAIN, 'zip')\n",
        "shutil.unpack_archive(ZIP_PATH_VAL, IMG_DIR_VAL, 'zip')\n",
        "shutil.unpack_archive(ZIP_PATH_TEST, IMG_DIR_TEST, 'zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiKyuXpSRBDo"
      },
      "source": [
        "# sets what device to run on\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and N_GPU > 0) else \"cpu\")\n",
        "print(f'Device: {device}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIvywFUpO6e-"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jNQKg1RQKBV"
      },
      "source": [
        "### Apply Mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX0otkZY1250"
      },
      "source": [
        "# helper function for apply the mask for cutting the frame to 4:3\n",
        "def apply_mask(img_batch):\n",
        "    if len(img_batch.shape) > 3:\n",
        "        img_batch[:, :, :, (IMG_WIDTH - SINGLE_SIDE):] = -1\n",
        "        img_batch[:, :, :, :SINGLE_SIDE] = -1\n",
        "    else:\n",
        "        img_batch[:, :, (IMG_WIDTH - SINGLE_SIDE):] = -1\n",
        "        img_batch[:, :, :SINGLE_SIDE] = -1\n",
        "    \n",
        "    return img_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwwxbF1_Vtp-"
      },
      "source": [
        "### Apply Padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtzaWgmIRXyE"
      },
      "source": [
        "# adds -1 padding to the sides of images that are in 4:3 aspect ratio\n",
        "def apply_padding(img):\n",
        "    padding = torch.zeros([3, IMG_HEIGHT, SINGLE_SIDE])\n",
        "    padding = padding.new_full((3, IMG_HEIGHT, SINGLE_SIDE), -1)\n",
        "    img_cat = torch.cat((padding, image, padding), 2)\n",
        "\n",
        "    return img_cat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2rycGHHtQkV"
      },
      "source": [
        "### Apply Composite"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STwlFxFytN_G"
      },
      "source": [
        "def apply_comp(img, img_gen):\n",
        "    if len(img.shape) > 3:\n",
        "        comp_img = torch.cat((img_gen[:, :, :, :SINGLE_SIDE], \n",
        "                            img[:, :, :, SINGLE_SIDE:(IMG_WIDTH-SINGLE_SIDE)], \n",
        "                            img_gen[:, :, :, (IMG_WIDTH-SINGLE_SIDE):]), 3)\n",
        "    else:\n",
        "        comp_img = torch.cat((img_gen[:, :, :SINGLE_SIDE], \n",
        "                    img[:, :, SINGLE_SIDE:(IMG_WIDTH-SINGLE_SIDE)], \n",
        "                    img_gen[:, :, (IMG_WIDTH-SINGLE_SIDE):]), 2)\n",
        "\n",
        "    return comp_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3EJizf_QNU9"
      },
      "source": [
        "### Image Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cb9jrXtGSLZD"
      },
      "source": [
        "# helper function for converting images to a normal range\n",
        "def img_scale(img_tensor, plot=False):\n",
        "    min_value = img_tensor.min()\n",
        "    span = img_tensor.max() - img_tensor.min()\n",
        "    img_tensor = (img_tensor - min_value) / span\n",
        "\n",
        "    # tensorboard and matplotlib take images in different formats\n",
        "    if plot:\n",
        "        img_tensor = img_tensor.transpose(0, 2)\n",
        "        img_tensor = torch.rot90(img_tensor, -1) # fixes a rotation issue\n",
        "\n",
        "    return img_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-0HhH4-QRx8"
      },
      "source": [
        "### Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6-o7ctT6Knh"
      },
      "source": [
        "def checkpoint(epoch, i, batch_counter, disc_loss, train_loss, val_loss):\n",
        "    print(f'Epoch: {epoch}/{N_EPOCHS}, Batch in Epoch: {i}/{N_BATCHES}, Total Images {batch_counter * BATCH_SIZE}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "    # saves everything once every epoch\n",
        "    if i % 600 == 0:\n",
        "        # saves loss to the tensorboard log\n",
        "        writer = SummaryWriter(LOG_DIR)\n",
        "        writer.add_scalar('Loss/Disc', disc_loss, batch_counter)\n",
        "        writer.add_scalar('Loss/Train', train_loss, batch_counter)\n",
        "        writer.add_scalar('Loss/Val', val_loss, batch_counter)\n",
        "\n",
        "        # saves a checkpoint\n",
        "        checkpoint = {'gen_state': gen.state_dict(), \n",
        "                        'gen_optimizer': optimizer_gen.state_dict()}\n",
        "        torch.save(checkpoint, LOG_DIR + 'checkpoint.pt')\n",
        "\n",
        "        # gets images from the dataloader\n",
        "        train_image = dataloader_train.dataset.__getitem__(TRAIN_REFERENCE_INDEX)\n",
        "        train_image = train_image.unsqueeze(0)\n",
        "        train_image = train_image.cuda()\n",
        "        train_image = img_scale(train_image)\n",
        "        val_image = dataloader_val.dataset.__getitem__(VAL_REFERENCE_INDEX)\n",
        "        val_image = val_image.unsqueeze(0)\n",
        "        val_image = val_image.cuda()\n",
        "        val_image = img_scale(val_image)\n",
        "        test_image = dataloader_test.dataset.__getitem__(TEST_REFERENCE_INDEX)\n",
        "        test_image = apply_padding(test_image)\n",
        "        test_image = test_image.unsqueeze(0)\n",
        "        test_image = test_image.cuda()\n",
        "        test_image = img_scale(test_image)\n",
        "\n",
        "        # if this is the first epoch we save the reference images\n",
        "        if epoch == 0:\n",
        "            print('Saving reference images')\n",
        "            # training reference image\n",
        "            writer.add_image(f'.Reference Train Image', train_image.squeeze(0))\n",
        "            writer.add_image(f'.Reference Train Image Mask', apply_mask(train_image.squeeze(0)))\n",
        "            # validation reference image\n",
        "            writer.add_image(f'.Reference Validation Image', val_image.squeeze(0))\n",
        "            writer.add_image(f'.Reference Validation Image Mask', apply_mask(val_image.squeeze(0)))\n",
        "            # testing reference image\n",
        "            writer.add_image(f'.Reference Test Image', test_image.squeeze(0))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            _, train_image_gen = gen(apply_mask(train_image))\n",
        "            _, val_image_gen = gen(apply_mask(val_image))\n",
        "            _, test_image_gen = gen(test_image)\n",
        "\n",
        "        train_image_gen = train_image_gen.squeeze(0)\n",
        "        train_image_gen = img_scale(train_image_gen)\n",
        "        train_image_gen = apply_comp(train_image.squeeze(0), train_image_gen)\n",
        "        writer.add_image(f'batch_{batch_counter}_train', train_image_gen)\n",
        "\n",
        "        val_image_gen = val_image_gen.squeeze(0)\n",
        "        val_image_gen = img_scale(val_image_gen)\n",
        "        val_image_gen = apply_comp(val_image.squeeze(0), val_image_gen)\n",
        "        writer.add_image(f'batch_{batch_counter}_val', val_image_gen)\n",
        "\n",
        "        test_image_gen = test_image_gen.squeeze(0)\n",
        "        test_image_gen = img_scale(test_image_gen)\n",
        "        test_image_gen = apply_comp(test_image.squeeze(0), test_image_gen)\n",
        "        writer.add_image(f'batch_{batch_counter}_test', test_image_gen)\n",
        "\n",
        "        # saves the epoch counter\n",
        "        with open(LOG_DIR + '/itercount.txt', 'w') as f:\n",
        "            f.write(str(batch_counter))\n",
        "        \n",
        "        writer.close()\n",
        "        print('Saved checkpoint')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4NfzEuXCccW"
      },
      "source": [
        "## Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u81vmXXZ4faK"
      },
      "source": [
        "# a custom dataset class for reading in our images from the list\n",
        "class ReadFromList(Dataset):\n",
        "    def __init__(self, img_list, transform=None):\n",
        "        self.img_list = img_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.img_list[idx]).convert('RGB')\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LHTb6ZeQnvJ"
      },
      "source": [
        "### Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DLEwB1B5Y-6"
      },
      "source": [
        "# gets the list of images\n",
        "img_list = glob.glob(IMG_DIR_TRAIN + '*')\n",
        "img_list.sort()\n",
        "print(f'Number of images: {len(img_list)}')\n",
        "img_list = img_list[:DATASET_SIZE]\n",
        "\n",
        "# makes the dataset and data loader\n",
        "dataset = ReadFromList(img_list, transform=transforms.Compose([\n",
        "                                    # transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),]))\n",
        "\n",
        "dataloader_train = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE//2, shuffle=True, num_workers=N_WORKERS)\n",
        "print(f'Size of dataset: {len(dataloader_train.dataset)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1O1ZhgZP5qek"
      },
      "source": [
        "# Plot some training images\n",
        "batch = next(iter(dataloader_train))\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "plt.axis('off')\n",
        "plt.title('Target Images')\n",
        "plt.imshow(np.transpose(vutils.make_grid(batch.to(device)[:64], padding=2, normalize=True, nrow=5).cpu(),(1,2,0)))\n",
        "\n",
        "batch = apply_mask(batch)\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "plt.axis('off')\n",
        "plt.title('Generator Input Images')\n",
        "plt.imshow(np.transpose(vutils.make_grid(batch.to(device)[:64], padding=2, normalize=True, nrow=5).cpu(),(1,2,0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHZx5AcOZ1mF"
      },
      "source": [
        "TRAIN_REFERENCE_INDEX = 200\n",
        "\n",
        "image = dataloader_train.dataset.__getitem__(TRAIN_REFERENCE_INDEX)\n",
        "image = img_scale(image)\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.axis('off')\n",
        "plt.title('Target Images')\n",
        "plt.imshow(np.transpose(vutils.make_grid(image).cpu(),(1,2,0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlqBC-BcRv00"
      },
      "source": [
        "### Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcbmakaAzjg1"
      },
      "source": [
        "# gets the list of images\n",
        "img_list = glob.glob(IMG_DIR_VAL + '*')\n",
        "img_list.sort()\n",
        "print(f'Number of images: {len(img_list)}')\n",
        "\n",
        "# makes the dataset and data loader\n",
        "dataset = ReadFromList(img_list, transform=transforms.Compose([\n",
        "                                    # transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),]))\n",
        "\n",
        "dataloader_val = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE//2, shuffle=True, num_workers=N_WORKERS)\n",
        "print(f'Size of dataset: {len(dataloader_val.dataset)}')\n",
        "\n",
        "# This is the testing reference image\n",
        "VAL_REFERENCE_INDEX = 100\n",
        "image = dataloader_val.dataset.__getitem__(VAL_REFERENCE_INDEX)\n",
        "image = img_scale(image)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.axis('off')\n",
        "plt.title('Reference Image')\n",
        "plt.imshow(np.transpose(vutils.make_grid(image).cpu(),(1,2,0)))\n",
        "\n",
        "image = apply_mask(image)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.axis('off')\n",
        "plt.title('Image with Map')\n",
        "plt.imshow(np.transpose(vutils.make_grid(image).cpu(),(1,2,0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82JG3e-KMnbV"
      },
      "source": [
        "### Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTy_-Z5sL9ve"
      },
      "source": [
        "# gets the list of images\n",
        "img_list = glob.glob(IMG_DIR_TEST + '*')\n",
        "img_list.sort()\n",
        "print(f'Number of images: {len(img_list)}')\n",
        "\n",
        "# makes the dataset and data loader\n",
        "dataset = ReadFromList(img_list, transform=transforms.Compose([\n",
        "                                    # transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),]))\n",
        "\n",
        "dataloader_test = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE//2, shuffle=True, num_workers=N_WORKERS)\n",
        "print(f'Size of dataset: {len(dataloader_test.dataset)}')\n",
        "\n",
        "# This is the testing reference image\n",
        "TEST_REFERENCE_INDEX = 20\n",
        "image = dataloader_test.dataset.__getitem__(TEST_REFERENCE_INDEX)\n",
        "image = img_scale(image)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.axis('off')\n",
        "plt.title('Reference Image')\n",
        "plt.imshow(np.transpose(vutils.make_grid(image, padding=2, nrow=4).cpu(),(1,2,0)))\n",
        "\n",
        "img_cat = apply_padding(image)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.axis('off')\n",
        "plt.title('Reference Image')\n",
        "plt.imshow(np.transpose(vutils.make_grid(img_cat, padding=2, nrow=4).cpu(),(1,2,0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rphdi8N1CiiX"
      },
      "source": [
        "## Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEc_2lcspsqF"
      },
      "source": [
        "### Generartor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA1BFkFiEjJG"
      },
      "source": [
        "# custom weights initialization called on netG and netD\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImbjCSqp7zZF"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        \n",
        "        self.dilconv1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, dilation=2, padding=2)\n",
        "        self.dilconv2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, dilation=4, padding=4)\n",
        "        self.dilconv3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, dilation=8, padding=8)\n",
        "        self.dilconv4 = nn.Conv2d(256, 256, kernel_size=3, stride=1, dilation=16, padding=16)\n",
        "\n",
        "        self.conv7 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.conv8 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        self.deconv1 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv9 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        \n",
        "        self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv10 = nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        \n",
        "        self.conv11 = nn.Conv2d(32, 3, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        self.batch_norm3 = nn.BatchNorm2d(3)\n",
        "        self.batch_norm32 = nn.BatchNorm2d(32)\n",
        "        self.batch_norm64 = nn.BatchNorm2d(64)\n",
        "        self.batch_norm128 = nn.BatchNorm2d(128)\n",
        "        self.batch_norm256 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, img):\n",
        "        # encoding\n",
        "        x = self.conv1(img)\n",
        "        x = self.batch_norm64(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.batch_norm128(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.batch_norm128(x)\n",
        "        x = self.relu(x)\n",
        "        \n",
        "        x = self.conv4(x)\n",
        "        x = self.batch_norm256(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv5(x)\n",
        "        x = self.batch_norm256(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv6(x)\n",
        "        x = self.batch_norm256(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # dilation\n",
        "        x = self.dilconv1(x)\n",
        "        x = self.batch_norm256(x)\n",
        "        x = self.relu(x)        \n",
        "\n",
        "        x = self.dilconv2(x)\n",
        "        x = self.batch_norm256(x)\n",
        "        x = self.relu(x)        \n",
        "\n",
        "        x = self.dilconv3(x)\n",
        "        x = self.batch_norm256(x)\n",
        "        x = self.relu(x)        \n",
        "\n",
        "        x = self.dilconv4(x)\n",
        "        x = self.batch_norm256(x)\n",
        "        x = self.relu(x)        \n",
        "\n",
        "        # more encoding\n",
        "        x = self.conv7(x)\n",
        "        x = self.batch_norm256(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv8(x)\n",
        "        x = self.batch_norm256(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # decoding\n",
        "        x = self.deconv1(x)\n",
        "        x = self.batch_norm128(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv9(x)\n",
        "        x = self.batch_norm128(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.deconv2(x)\n",
        "        x = self.batch_norm64(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.conv10(x)\n",
        "        x = self.batch_norm32(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # output\n",
        "        x = self.conv11(x)\n",
        "        x = self.tanh(x)\n",
        "\n",
        "        # creates the local output and global output that is combined with the orginal image\n",
        "        gen_output_local = x[:, :, :, (IMG_WIDTH - SINGLE_SIDE):]\n",
        "        global_gen_output = x\n",
        "\n",
        "        return (gen_output_local, x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mwnu0pQYls4V"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ3BlMuaUOXn"
      },
      "source": [
        "### Initialize the Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtE6YhY53FrI"
      },
      "source": [
        "gen = Generator(N_GPU).to(device)\n",
        "gen.apply(weights_init)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD65EPG-ZqaY"
      },
      "source": [
        "### Loss Function and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNpFXQTnZuh_"
      },
      "source": [
        "loss = nn.BCELoss()\n",
        "mse_loss = nn.MSELoss()\n",
        "optimizer_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.9))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZBU8zSGnHJb"
      },
      "source": [
        "### Loads Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63H_ZHSZmFfL"
      },
      "source": [
        "if CHECKPOINT_TYPE == 'prev_checkpoint':\n",
        "    # loads the model weights\n",
        "    saved_checkpoint = torch.load(LOG_DIR + 'checkpoint.pt')\n",
        "    gen.load_state_dict(saved_checkpoint['gen_state'])\n",
        "    optimizer_gen.load_state_dict(saved_checkpoint['gen_optimizer'])\n",
        "    print('Checkpoint Loaded')\n",
        "    \n",
        "    # loads the epoch counter\n",
        "    with open(LOG_DIR + 'itercount.txt', 'r') as f:\n",
        "        batch_counter = int(f.read())\n",
        "    # moves it up one becuase it's currenlty at the last epoch we did\n",
        "    batch_counter += 1\n",
        "\n",
        "elif CHECKPOINT_TYPE == 'none':\n",
        "    # remove all previous logs\n",
        "    try:\n",
        "        shutil.rmtree(LOG_DIR)\n",
        "        print('Folders removed')\n",
        "    except FileNotFoundError:\n",
        "        print('No log folder found')\n",
        "\n",
        "    batch_counter = 1\n",
        "\n",
        "else:\n",
        "    print('Failed to specify a type')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_lWXZ2i6cbd"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgZ-sLbSvWPv"
      },
      "source": [
        "for epoch in range(N_EPOCHS):\n",
        "    # gets data for the generator\n",
        "    for i, batch in enumerate(dataloader_train, 0):\n",
        "\n",
        "        # generater\n",
        "        gen.zero_grad()\n",
        "        batch_mask = batch.clone()\n",
        "        batch_mask = apply_mask(batch_mask)\n",
        "        _, gen_output_global = gen(batch_mask.to(device))\n",
        "\n",
        "        # keeps only the edges from the generator and calculates the loss\n",
        "        gen_output_global = apply_comp(batch.to(device), gen_output_global)\n",
        "        train_loss = mse_loss(gen_output_global, batch.to(device))\n",
        "        \n",
        "        # error and optimize\n",
        "        train_loss.backward()\n",
        "        optimizer_gen.step()\n",
        "\n",
        "        # checkpoints and print statistics every n batches\n",
        "        if i % 10 == 0:\n",
        "            # gets the testing MSE\n",
        "            batch = next(iter(dataloader_val))\n",
        "            batch_mask = batch.clone()\n",
        "            batch_mask = apply_mask(batch_mask)\n",
        "            with torch.no_grad():\n",
        "                _, gen_output_global = gen(batch_mask.to(device))\n",
        "            gen_output_global = apply_comp(batch.to(device), gen_output_global)\n",
        "\n",
        "            val_loss = mse_loss(gen_output_global, batch.to(device))\n",
        "            checkpoint(epoch, i, batch_counter, 0, train_loss.item(), val_loss.item())\n",
        "\n",
        "        batch_counter += 1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}