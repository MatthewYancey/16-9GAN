{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_lama_default.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMdPPN8P07zrU9QJVtl/vOl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatthewYancey/GANime/blob/master/src/train_lama_default.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tain LaMa\n",
        "This notebok runs the default training for LaMa"
      ],
      "metadata": {
        "id": "iVrGyIPupkW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/detectron2 detectron2_repo\n",
        "!pip install -e detectron2_repo"
      ],
      "metadata": {
        "id": "vTaQAZl0wSFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "3P4-_KddMXYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir = '/content/lama/my_dataset/'\n",
        "train_zip = '/content/gdrive/MyDrive/repos/GANime/data_out/pokemon/train.zip'\n",
        "val_zip = '/content/gdrive/MyDrive/repos/GANime/data_out/pokemon/validate.zip'\n",
        "test_zip = '/content/gdrive/MyDrive/repos/GANime/data_out/pokemon/test.zip'"
      ],
      "metadata": {
        "id": "4YQmQ5TLe-2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\n> Cloning the repo')\n",
        "!git clone https://github.com/saic-mdal/lama.git\n",
        "\n",
        "print('\\n> Install dependencies')\n",
        "!pip install -r lama/requirements.txt --quiet\n",
        "!pip install wget --quiet\n",
        "\n",
        "print('\\n> Changing the dir to:')\n",
        "% cd /content/lama\n",
        "\n",
        "# downloads the models for evaluations\n",
        "print('Downloading resnet model for evaluation')\n",
        "!mkdir -p ade20k/ade20k-resnet50dilated-ppm_deepsup/\n",
        "!wget -P ade20k/ade20k-resnet50dilated-ppm_deepsup/ http://sceneparsing.csail.mit.edu/model/pytorch/ade20k-resnet50dilated-ppm_deepsup/encoder_epoch_20.pth\n",
        "\n",
        "print('>fixing opencv')\n",
        "!pip uninstall opencv-python-headless -y --quiet\n",
        "!pip install opencv-python-headless==4.1.2.30 --quiet\n",
        "\n",
        "print('\\n> Init mask-drawing code')\n",
        "import base64, os\n",
        "from IPython.display import HTML, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from shutil import copyfile\n",
        "import shutil\n",
        "import cv2"
      ],
      "metadata": {
        "id": "ACQUc8OQT4uH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# copy the dataset set over\n",
        "try:\n",
        "    os.mkdir(dataset_dir)\n",
        "except FileExistsError:\n",
        "    pass\n",
        "\n",
        "print('Copying and unpacking training data')\n",
        "shutil.copy(train_zip, dataset_dir + 'train.zip')\n",
        "shutil.unpack_archive(dataset_dir + 'train.zip', dataset_dir + '/train/')\n",
        "print('Copying and unpacking validation data')\n",
        "shutil.copy(val_zip, dataset_dir + 'val.zip')\n",
        "shutil.unpack_archive(dataset_dir + 'val.zip', dataset_dir + '/val_original/')\n",
        "print('Copying and unpacking testing data')\n",
        "shutil.copy(test_zip, dataset_dir + 'test.zip')\n",
        "shutil.unpack_archive(dataset_dir + 'test.zip', dataset_dir + '/visual_test_original/')"
      ],
      "metadata": {
        "id": "AgZ2BaVXkede"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# makes the config file for training\n",
        "!touch my_dataset.yaml\n",
        "!echo \"data_root_dir: $(pwd)/my_dataset/\" >> my_dataset.yaml\n",
        "!echo \"out_root_dir: $/content/gdrive/MyDrive/repos/GANime/data_out/lama_model/\" >> my_dataset.yaml\n",
        "!echo \"tb_dir: $(pwd)/tb_logs/\" >> my_dataset.yaml\n",
        "!mv my_dataset.yaml ${PWD}/configs/training/location/\n",
        "\n",
        "# change /content/lama/configs/training/trainer/any_gpu_large_ssim_ddp_final.yaml limit_train_batches: 2600 # was 25000\n",
        "# shutil.copy('/content/gdrive/MyDrive/repos/GANime/src/any_gpu_large_ssim_ddp_final.yaml', '/content/lama/configs/training/trainer/any_gpu_large_ssim_ddp_final.yaml')"
      ],
      "metadata": {
        "id": "FJbZWmtanNZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# produces the masks\n",
        "def prep_data(in_dir, out_dir, img_width, img_height, single_side):\n",
        "    # creates a test set of validaiton\n",
        "    if os.path.isdir(out_dir):\n",
        "        shutil.rmtree(out_dir)\n",
        "    os.mkdir(out_dir)\n",
        "\n",
        "    img_paths = glob.glob(in_dir + '*')\n",
        "\n",
        "    # makes a mask and saves it to correspond with the names of images\n",
        "    inside = np.zeros((img_height, img_width - 2*single_side, 3), np.uint8)\n",
        "    outsides = np.ones((img_height, single_side, 3), np.uint8) * 255\n",
        "    mask = cv2.hconcat([outsides, inside, outsides])\n",
        "\n",
        "    for img_path in img_paths:\n",
        "        # converts image\n",
        "        img = cv2.imread(img_path)\n",
        "        new_img_path = out_dir + os.path.basename(img_path).replace('.jpg', '.png')\n",
        "        cv2.imwrite(new_img_path, img)\n",
        "\n",
        "        # saves mask\n",
        "        mask_name = new_img_path.replace('.png', '') + '_mask.png'\n",
        "        cv2.imwrite(mask_name, mask)"
      ],
      "metadata": {
        "id": "J6WThqs3IL0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prep_data(dataset_dir + 'val_original/', dataset_dir + 'val/', 512, 288, 64)\n",
        "prep_data(dataset_dir + 'visual_test_original/', dataset_dir + 'visual_test/', 512, 288, 64)"
      ],
      "metadata": {
        "id": "XUjl1RUbUIWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!export TORCH_HOME=$(pwd) && export USER='matt' && export PYTHONPATH=. && python3 bin/train.py -cn lama-fourier location=my_dataset data.batch_size=10\n",
        "# !cat ${PWD}/configs/training/data/abl-04-256-mh-dist"
      ],
      "metadata": {
        "id": "ZUQVVnILn7H1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shutil.make_archive('/content/gdrive/MyDrive/lama_model', 'zip', '/content/lama/experiments/matt_2022-02-12_20-17-09_train_lama-fourier_')"
      ],
      "metadata": {
        "id": "3XSnUHHOK_59"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}